# L·ªói trong Anchor Routing Implementation

## üî¥ L·ªói nghi√™m tr·ªçng 1: Khi task b·ªã skip, logits kh√¥ng ƒë∆∞·ª£c th√™m v√†o

**V·ªã tr√≠**: D√≤ng 414-415 trong `continual_clip_variational.py`

```python
if n_samples_task == 0:
    continue  # ‚ùå Task b·ªã skip ho√†n to√†n!
```

**V·∫•n ƒë·ªÅ**:

- Khi m·ªôt task c√≥ `alloc[i] = 0`, task ƒë√≥ b·ªã skip ho√†n to√†n
- `logits.append(logits_)` kh√¥ng ƒë∆∞·ª£c g·ªçi cho task ƒë√≥
- Khi `logits = torch.cat(logits, -1)` ·ªü d√≤ng 438, s·ªë l∆∞·ª£ng logits s·∫Ω √≠t h∆°n s·ªë l∆∞·ª£ng task
- **Shape kh√¥ng ƒë√∫ng**: V√≠ d·ª• n·∫øu c√≥ 3 task nh∆∞ng task 1 b·ªã skip, logits s·∫Ω ch·ªâ c√≥ 2 ph·∫ßn, d·∫´n ƒë·∫øn l·ªói shape mismatch

**Fix**:

```python
# Thay v√¨ skip, ph·∫£i th√™m logits v·ªõi gi√° tr·ªã zero ho·∫∑c -inf
if n_samples_task == 0:
    # T·∫°o logits zero cho task n√†y ƒë·ªÉ gi·ªØ shape
    num_classes_task = self.task_to_cls_num[i]
    logits_ = torch.zeros((image_features_normed.shape[0], num_classes_task),
                          device=image_features_normed.device, dtype=image_features_normed.dtype)
    logits_.fill_(-float('inf'))  # Ho·∫∑c d√πng gi√° tr·ªã r·∫•t nh·ªè
    logits.append(logits_)
    continue
```

---

## üî¥ L·ªói nghi√™m tr·ªçng 2: taskwise_means thi·∫øu khi task b·ªã skip

**V·ªã tr√≠**: D√≤ng 558-574 trong training mode

**V·∫•n ƒë·ªÅ**:

- Khi task b·ªã skip, `taskwise_means.append(rsamples.mean(0))` kh√¥ng ƒë∆∞·ª£c g·ªçi
- ·ªû d√≤ng 606, `taskwise_means = torch.cat(taskwise_means)` s·∫Ω l·ªói n·∫øu s·ªë l∆∞·ª£ng taskwise_means kh√¥ng ƒë√∫ng

**Fix**:

```python
if n_samples_task == 0:
    # Ph·∫£i append m·ªôt gi√° tr·ªã gi·∫£ cho taskwise_means
    taskwise_means.append(torch.zeros_like(text_features_relevant.mean(0)))
    # ... ph·∫ßn logits nh∆∞ tr√™n
    continue
```

---

## üü° L·ªói ti·ªÅm ·∫©n 3: Allocation kh√¥ng ch√≠nh x√°c

**V·ªã tr√≠**: D√≤ng 378, 511

```python
alloc = (avg_w * self.forward_times).round().to(torch.int64)
```

**V·∫•n ƒë·ªÅ**:

- T·ªïng c·ªßa `alloc` c√≥ th·ªÉ kh√¥ng b·∫±ng `forward_times`
- C√≥ th·ªÉ nh·ªè h∆°n (n·∫øu nhi·ªÅu task c√≥ weight nh·ªè) ho·∫∑c l·ªõn h∆°n (n·∫øu rounding)
- D·∫´n ƒë·∫øn s·ªë l∆∞·ª£ng samples kh√¥ng ƒë√∫ng nh∆∞ mong ƒë·ª£i

**Fix**:

```python
# ƒê·∫£m b·∫£o t·ªïng b·∫±ng forward_times
alloc = (avg_w * self.forward_times).round().to(torch.int64)
total = alloc.sum().item()
if total != self.forward_times:
    # ƒêi·ªÅu ch·ªânh task c√≥ weight cao nh·∫•t
    diff = self.forward_times - total
    top_idx = torch.argmax(avg_w).item()
    alloc[top_idx] += diff
```

---

## üü° L·ªói ti·ªÅm ·∫©n 4: Image anchor ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o trong test

**V·ªã tr√≠**: D√≤ng 345, 269-294

**V·∫•n ƒë·ªÅ**:

- Image anchor ch·ªâ ƒë∆∞·ª£c update khi c√≥ labels (d√≤ng 345: `if labels is not None`)
- Trong test mode, c√≥ th·ªÉ kh√¥ng c√≥ labels
- Khi g·ªçi `_get_anchor_weights`, image anchor c√≥ th·ªÉ ch∆∞a c√≥ ‚Üí d√πng fallback (mean c·ªßa batch), kh√¥ng ch√≠nh x√°c

**Fix**:

```python
# Trong test mode, c≈©ng c·∫ßn update anchor n·∫øu c√≥ th·ªÉ
if self.use_anchor_routing:
    if labels is not None:
        self._update_image_anchors(image_features_normed, labels)
    # Ho·∫∑c update d·ª±a tr√™n prediction n·∫øu kh√¥ng c√≥ labels
```

---

## üü° L·ªói ti·ªÅm ·∫©n 5: compute_ram thi·∫øu khi task b·ªã skip

**V·ªã tr√≠**: D√≤ng 436, 600

**V·∫•n ƒë·ªÅ**:

- Khi task b·ªã skip, `samplewise_text_feats.append(text_features_relevant)` kh√¥ng ƒë∆∞·ª£c g·ªçi
- C√≥ th·ªÉ d·∫´n ƒë·∫øn l·ªói shape khi compute RAM

**Fix**: T∆∞∆°ng t·ª± nh∆∞ logits, ph·∫£i append gi√° tr·ªã gi·∫£

---

## ‚úÖ Gi·∫£i ph√°p t·ªïng th·ªÉ

**Thay v√¨ skip task ho√†n to√†n, n√™n ƒë·∫£m b·∫£o m·ªói task c√≥ √≠t nh·∫•t 1 sample:**

```python
# ƒê·∫£m b·∫£o m·ªói task c√≥ √≠t nh·∫•t 1 sample
if alloc.sum().item() == 0:
    top = torch.argmax(avg_w).item()
    alloc[top] = 1
else:
    # ƒê·∫£m b·∫£o task c√≥ trong batch c√≥ √≠t nh·∫•t 1 sample
    for ti, (lo, hi) in enumerate(bounds):
        if ((labels >= lo) & (labels < hi)).any():
            if alloc[ti].item() == 0:
                alloc[ti] = 1

    # ƒêi·ªÅu ch·ªânh ƒë·ªÉ t·ªïng v·∫´n ~ forward_times
    total = alloc.sum().item()
    if total > self.forward_times:
        # Gi·∫£m samples c·ªßa task c√≥ weight th·∫•p nh·∫•t
        while total > self.forward_times and alloc.min().item() > 1:
            min_idx = torch.argmin(alloc).item()
            alloc[min_idx] -= 1
            total -= 1
    elif total < self.forward_times:
        # TƒÉng samples c·ªßa task c√≥ weight cao nh·∫•t
        max_idx = torch.argmax(alloc).item()
        alloc[max_idx] += (self.forward_times - total)
```

**Ho·∫∑c ƒë∆°n gi·∫£n h∆°n: ƒê·∫£m b·∫£o kh√¥ng bao gi·ªù c√≥ n_samples_task == 0:**

```python
n_samples_task = int(alloc[i].item()) if (self.use_anchor_routing and alloc is not None) else self.forward_times
n_samples_task = max(1, n_samples_task)  # ‚úÖ ƒê·∫£m b·∫£o √≠t nh·∫•t 1 sample
```

---

## üìù T√≥m t·∫Øt

1. **L·ªói nghi√™m tr·ªçng nh·∫•t**: Khi task b·ªã skip, logits kh√¥ng ƒë∆∞·ª£c th√™m ‚Üí shape mismatch
2. **L·ªói nghi√™m tr·ªçng th·ª© 2**: taskwise_means thi·∫øu ‚Üí l·ªói khi cat
3. **L·ªói ti·ªÅm ·∫©n**: Allocation kh√¥ng ch√≠nh x√°c, image anchor ch∆∞a kh·ªüi t·∫°o

**Gi·∫£i ph√°p ƒë∆°n gi·∫£n nh·∫•t**: ƒê·∫£m b·∫£o `n_samples_task >= 1` cho m·ªçi task, kh√¥ng bao gi·ªù skip task ho√†n to√†n.
